{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S.I Adams - 3838305\n",
    "# CSC311 - Back-Propagation Assignment\n",
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np \n",
    "      \n",
    "# Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "X = np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y = np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "# Class definition\n",
    "class NN:\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.w1 = np.random.rand(self.x.shape[1],4) # randonly generated weights\n",
    "        self.w2 = np.random.rand(4,1) # randonly generated weights\n",
    "        self.y = y\n",
    "        self.output = np.zeros(y.shape)\n",
    "    \n",
    "    # Activation function\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Derivative of sigmoid\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # FeedForward algorithm feeds the input data forward through the layers\n",
    "    # outputs the predicted output \n",
    "    def feedforward(self):\n",
    "        # Dot product of inputs and weights and passing it through the activation function\n",
    "        self.layer1 = sigmoid(np.dot(self.x, self.w1))\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.w2))\n",
    "        return self.layer2\n",
    "\n",
    "    # mean sum squared error value\n",
    "    def back_propagate_error(self):\n",
    "        return np.mean(np.square(y - self.feedforward()))\n",
    "\n",
    "    # Propagates backwards when predicted output is not equal to expected output\n",
    "    # by updating the weights using gradient decient until error value is extremly small \n",
    "    # and predicted output is equal to expected output   \n",
    "    def backprop(self):\n",
    "        self.w1 += np.dot(np.transpose(self.x), np.dot(2 * (self.y - self.output) * sigmoid_derivative(self.output), np.transpose(self.w2)) * sigmoid_derivative(self.layer1))\n",
    "        self.w2 += np.dot(np.transpose(self.layer1), 2 * (self.y - self.output) * sigmoid_derivative(self.output))\n",
    "        \n",
    "    # Trains the model by continuosly feeding the data forward and backwards until\n",
    "    # prediction is accurate \n",
    "    def train(self, X, y):\n",
    "        self.output = self.feedforward()\n",
    "        self.backprop()\n",
    "\n",
    "NN1 = NN(X,y)\n",
    "for i in range(1000): # trains the NN\n",
    "    if i % 100 == 0: # check if i mod 100 is equal to 0\n",
    "        print (\"Actual Output: \\n\", str(y))\n",
    "        print (\"Predicted Output: \\n\", str(NN1.feedforward()))\n",
    "        print (\"Error value: \\n\", str(NN1.back_propagate_error())) \n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN1.train(X, y)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Weights:\n",
    "# Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.\n",
    "# weights associated with neuron connections must be updated after forward passes of data through the network\n",
    "# The error represents the difference between actual output and predicted output values\n",
    "# This error is required at neurons to make weight adjustments, and are propagated backward through the network after calculation of the backpropagation error. \n",
    "# This is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
    "# Gradient descent is used to more efficiently determine optimal weights by acting as a guide when searching for a cost function's optimal value\n",
    "# Stochastic gradient descent is a randomization of data sampling on which a single selection is used for error backpropagation (and weight updates)\n",
    "# Weight = weight + learning rate * error * input\n",
    "# Error is the neuron delta weight update changes that are needed\n",
    "\n",
    "# Update network weights with error\n",
    "# takes in 3 args, network, row and learning rate\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)): # loops through length of network number of times\n",
    "\t\tinputs = row[:-1] # sets input to all values in row except the last col in the row\n",
    "\t\tif i != 0: # if i is not equal to 0 \n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]] \n",
    "\t\tfor neuron in network[i]: # loops through each element in network\n",
    "\t\t\tfor j in range(len(inputs)):  # loops through length of input number of times\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j] # updating weights\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta'] # updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/python3\n",
    "import random\n",
    "from sklearn.cluster import KMeans as kmeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "DATAPOINTS = 20\n",
    "studentno = int(input('Please enter your student number: ')) # my student number is 3838305\n",
    "random.seed(studentno % 10000)\n",
    "col1 = [random.randint(0,studentno) for i in range(0,DATAPOINTS)]\n",
    "\n",
    "random.seed(studentno % 1000)\n",
    "col2 = [random.randint(0,studentno) for i in range(0,DATAPOINTS)]\n",
    "\n",
    "random.seed(studentno % 100)\n",
    "col3 = [random.randint(0,studentno) for i in range(0,DATAPOINTS)]\n",
    "\n",
    "random.seed(studentno % 10)\n",
    "col4 = [random.randint(0,studentno) for i in range(0,DATAPOINTS)]\n",
    "\n",
    "random.seed(studentno % 100000)\n",
    "col5 = [random.randint(0,studentno) for i in range(0,DATAPOINTS)]\n",
    "\n",
    "col6 = [i % 3 for i in range(0,DATAPOINTS)]\n",
    "\n",
    "sdata = open('data.txt', mode = 'w')\n",
    "[print(\"%s, %s, %s, %s, %s, %s\" % (str(col1[i]), str(col2[i]), str(col3[i]), str(col4[i]), str(col5[i]), str(col6[i])), file =sdata) for i in range(0,DATAPOINTS)]\n",
    "sdata.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scores: [25.0, 0.0, 25.0, 25.0, 50.0]\nMean Accuracy: 25.000%\n"
    }
   ],
   "source": [
    "# Backprop on the Seeds Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "\treturn stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)-1):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "# takes in 3 args, network, row and learning rate\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)): # loops through length of network number of times\n",
    "\t\tinputs = row[:-1] # sets input to all values in row except the last col in the row\n",
    "\t\tif i != 0: # if i is not equal to 0 \n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]] \n",
    "\t\tfor neuron in network[i]: # loops through each element in network\n",
    "\t\t\tfor j in range(len(inputs)):  # loops through length of input number of times\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j] # updating weights\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta'] # updating weights\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "\tn_inputs = len(train[0]) - 1\n",
    "\tn_outputs = len(set([row[-1] for row in train]))\n",
    "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(network, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'data.txt'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "sdata = open('back-prop-scores.txt', mode = 'w')\n",
    "print('Scores: %s' % scores, file =sdata)\n",
    "sdata.close()\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bit715fffe35f214ea4b9812c9e40debe9a",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}